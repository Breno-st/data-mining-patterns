{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "\n",
    "sys.path.append('/gspan_mining')\n",
    "from gspan_mining import gSpan\n",
    "from gspan_mining import GraphDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PatternGraphs:\n",
    "    \"\"\"\n",
    "    This template class is used to define a task for the gSpan implementation.\n",
    "    You should not modify this class but extend it to define new tasks\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, database):\n",
    "        # A list of subsets of graph identifiers.\n",
    "        # Is used to specify different groups of graphs (classes and training/test sets).\n",
    "        # The gid-subsets parameter in the pruning and store function will contain for each subset, all the occurrences\n",
    "        # in which the examined pattern is present.\n",
    "        self.gid_subsets = []\n",
    "\n",
    "        self.database = database  # A graphdatabase instance: contains the data for the problem.\n",
    "\n",
    "    def store(self, dfs_code, gid_subsets):\n",
    "        \"\"\"\n",
    "        Code to be executed to store the pattern, if desired.\n",
    "        The function will only be called for patterns that have not been pruned.\n",
    "        In correlated pattern mining, we may prune based on confidence, but then check further conditions before storing.\n",
    "        :param dfs_code: the dfs code of the pattern (as a string).\n",
    "        :param gid_subsets: the cover (set of graph ids in which the pattern is present) for each subset in self.gid_subsets\n",
    "        \"\"\"\n",
    "        print(\"Please implement the store function in a subclass for a specific mining task!\")\n",
    "\n",
    "    def prune(self, gid_subsets):\n",
    "        \"\"\"\n",
    "        prune function: used by the gSpan algorithm to know if a pattern (and its children in the search tree)\n",
    "        should be pruned.\n",
    "        :param gid_subsets: A list of the cover of the pattern for each subset.\n",
    "        :return: true if the pattern should be pruned, false otherwise.\n",
    "        \"\"\"\n",
    "        print(\"Please implement the prune function in a subclass for a specific mining task!\")\n",
    "\n",
    "\n",
    "class FrequentPositiveGraphs(PatternGraphs):\n",
    "    \"\"\"\n",
    "    Finds the frequent (support >= minsup) subgraphs among the positive graphs.\n",
    "    This class provides a method to build a feature matrix for each subset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, minsup, database, subsets, k):\n",
    "        \"\"\"\n",
    "        Initialize the task.\n",
    "        :param minsup: the minimum positive support\n",
    "        :param database: the graph database\n",
    "        :param subsets: the subsets (train and/or test sets for positive and negative class) of graph ids.\n",
    "        \"\"\"\n",
    "        super().__init__(database)\n",
    "        self.patterns = []  # The patterns found in the end (as dfs codes represented by strings) with their cover (as a list of graph ids).\n",
    "        self.minsup = minsup\n",
    "        self.sub = subsets\n",
    "        self.gid_subsets = [subsets[0], subsets[2]]\n",
    "        self.k = k\n",
    "        self.confidence_frequence = set()\n",
    "\n",
    "    # Stores any pattern found that has not been pruned\n",
    "    def store(self, dfs_code, gid_subsets):\n",
    "\n",
    "        confidence = len(gid_subsets[0])/ (len(gid_subsets[0]) + len(gid_subsets[1]))\n",
    "        frequence = len(gid_subsets[0]) + len(gid_subsets[1])\n",
    "        toProcess = False\n",
    "        notToProcess = False\n",
    "\n",
    "        if len(self.confidence_frequence) < self.k :\n",
    "            self.patterns.append((dfs_code, gid_subsets))\n",
    "            self.confidence_frequence.add((confidence, frequence))\n",
    "        else:\n",
    "            for conf, freq in self.confidence_frequence:\n",
    "                if conf == confidence:\n",
    "                    if freq == frequence:\n",
    "                        self.patterns.append((dfs_code, gid_subsets))\n",
    "                        notToProcess = True\n",
    "                    if freq < frequence:\n",
    "                        toProcess = True\n",
    "                if conf < confidence:\n",
    "                    toProcess = True\n",
    "\n",
    "            if(toProcess and notToProcess == False):\n",
    "                minConf, minFreq = self.getMinConfFreq()\n",
    "                for code, subset in self.patterns:\n",
    "                    if(len(subset[0])/ (len(subset[0]) + len(subset[1]))) == minConf and (len(subset[0]) + len(subset[1])) == minFreq:\n",
    "                        self.patterns.remove((code,subset))\n",
    "                self.confidence_frequence.remove((minConf,minFreq))\n",
    "                self.patterns.append((dfs_code, gid_subsets))\n",
    "                self.confidence_frequence.add((confidence, frequence))\n",
    "\n",
    "\n",
    "\n",
    "    def getMinConfFreq(self):\n",
    "        for minConf, minfreq in self.confidence_frequence:\n",
    "            break\n",
    "        for conf, freq in self.confidence_frequence:\n",
    "            if conf == minConf and minfreq > freq:\n",
    "                minfreq = freq\n",
    "            if conf < minConf:\n",
    "                minConf = conf\n",
    "                minfreq = freq\n",
    "\n",
    "        return minConf,minfreq\n",
    "\n",
    "\n",
    "    # Prunes any pattern that is not frequent in the positive class\n",
    "    def prune(self, gid_subsets):\n",
    "        # first subset is the set of positive ids\n",
    "        return (len(gid_subsets[0]) + len(gid_subsets[1])) < self.minsup\n",
    "\n",
    "    # creates a column for a feature matrix\n",
    "    def create_fm_col(self, all_gids, subset_gids):\n",
    "        subset_gids = set(subset_gids)\n",
    "        bools = []\n",
    "        for i, val in enumerate(all_gids):\n",
    "            if val in subset_gids:\n",
    "                bools.append(1)\n",
    "            else:\n",
    "                bools.append(0)\n",
    "        return bools\n",
    "\n",
    "    # return a feature matrix for each subset of examples, in which the columns correspond to patterns\n",
    "    # and the rows to examples in the subset.\n",
    "    def get_feature_matrices(self):\n",
    "        matrices = [[] for _ in self.sub]\n",
    "        count = 0\n",
    "        for pattern, gid_subsets in self.patterns:\n",
    "            for i, gid_subset in enumerate(gid_subsets):\n",
    "                matrices[i].append(self.create_fm_col(self.gid_subsets[i], gid_subset))\n",
    "\n",
    "            count = count+1\n",
    "        return [numpy.array(matrix).transpose() for matrix in matrices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task2(database_file_name_pos, database_file_name_neg ,k, minsup, nfolds):\n",
    "\n",
    "    if not os.path.exists(database_file_name_pos):\n",
    "        print('{} does not exist.'.format(database_file_name_pos))\n",
    "        sys.exit()\n",
    "    if not os.path.exists(database_file_name_neg):\n",
    "        print('{} does not exist.'.format(database_file_name_neg))\n",
    "        sys.exit()\n",
    "\n",
    "    graph_database = GraphDatabase()  # Graph database object\n",
    "    pos_ids = graph_database.read_graphs(database_file_name_pos)  # Reading positive graphs, adding them to database and getting ids\n",
    "    neg_ids = graph_database.read_graphs(database_file_name_neg)  # Reading negative graphs, adding them to database and getting ids\n",
    "    #print(graph_database._graphs[0].plot())\n",
    "\n",
    "\n",
    "    # If less than two folds: using the same set as training and test set (note this is not an accurate way to evaluate the performances!)\n",
    "    if nfolds < 2:\n",
    "        subsets = [\n",
    "            pos_ids,  # Positive training set\n",
    "            pos_ids,  # Positive test set\n",
    "            neg_ids,  # Negative training set\n",
    "            neg_ids  # Negative test set\n",
    "        ]\n",
    "        # Printing fold number:\n",
    "        print('fold {}'.format(1))\n",
    "        train_and_evaluate(minsup, graph_database, subsets)\n",
    "\n",
    "    # Otherwise: performs k-fold cross-validation:\n",
    "    else:\n",
    "        pos_fold_size = len(pos_ids) // nfolds\n",
    "        neg_fold_size = len(neg_ids) // nfolds\n",
    "        for i in range(nfolds):\n",
    "            # Use fold as test set, the others as training set for each class;\n",
    "            # identify all the subsets to be maintained by the graph mining algorithm.\n",
    "            subsets = [\n",
    "                numpy.concatenate((pos_ids[:i * pos_fold_size], pos_ids[(i + 1) * pos_fold_size:])),  # Positive training set\n",
    "                pos_ids[i * pos_fold_size:(i + 1) * pos_fold_size],  # Positive test set\n",
    "                numpy.concatenate((neg_ids[:i * neg_fold_size], neg_ids[(i + 1) * neg_fold_size:])),  # Negative training set\n",
    "                neg_ids[i * neg_fold_size:(i + 1) * neg_fold_size],  # Negative test set\n",
    "            ]\n",
    "            # Printing fold number:\n",
    "            print('fold {}'.format(i+1))\n",
    "            train_and_evaluate(minsup, graph_database, subsets, k)\n",
    "\n",
    "\n",
    "def maxFreq(pat, boundfreq):\n",
    "    biggest = 0\n",
    "    result = list()\n",
    "    for pattern in pat:\n",
    "        if (pattern[1] + pattern[2]) == biggest:\n",
    "            result.append(pattern)\n",
    "\n",
    "        if (pattern[1] + pattern[2]) > biggest and (pattern[1] + pattern[2]) < boundfreq:\n",
    "            result = [pattern]\n",
    "            biggest = pattern[1] + pattern[2]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def maxconf(task, bound):\n",
    "\tpat = list() # elements of patttern,pos,neg,confidence\n",
    "\tconf = 0\n",
    "\tsupp = 0\n",
    "\tfor pattern, gid_subsets in task.patterns:\n",
    "\t\tpos_support = len(gid_subsets[0])\n",
    "\t\tneg_support = len(gid_subsets[1])\n",
    "\t\tconfidence = pos_support / (pos_support + neg_support)\n",
    "\n",
    "\t\tif(confidence == conf):\n",
    "\t\t\tif (supp < pos_support + neg_support):\n",
    "\t\t\t\tpat = [[pattern,pos_support,neg_support]]\n",
    "\t\t\t\tconf = confidence\n",
    "\t\t\t\tsupp = pos_support + neg_support\n",
    "\n",
    "\t\t\tif(supp == pos_support + neg_support):\n",
    "\t\t\t\tpat.append([pattern,pos_support,neg_support])\n",
    "\n",
    "\t\tif(confidence > conf and confidence < bound):\n",
    "\t\t\tpat = [[pattern,pos_support,neg_support]]\n",
    "\t\t\tconf = confidence\n",
    "\t\t\tsupp = pos_support + neg_support\n",
    "\n",
    "\treturn pat\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate(minsup, database, subsets, k):\n",
    "    subsetsTrain = [subsets[0], subsets[2]]\n",
    "    subsetsTest = numpy.concatenate([subsets[1],subsets[3]])\n",
    "    task = FrequentPositiveGraphs(minsup, database, subsets, k)  # Creating task\n",
    "    taskTest = FrequentPositiveGraphs(minsup,database,subsets,k)\n",
    "\n",
    "\n",
    "    gSpan(task).run()  # Running gSpan\n",
    "\n",
    "    listPattern = list()\n",
    "    for i, j in task.patterns:\n",
    "            listPattern.append(i)\n",
    "    gSpan(taskTest).run()\n",
    "\n",
    "    # Creating feature matrices for training and testing:\n",
    "    featuresTrain = task.get_feature_matrices()\n",
    "\n",
    "    featuresTest = taskTest.get_feature_matrices()\n",
    "    train_fm = numpy.concatenate((featuresTrain[0], featuresTrain[1]))  # Training feature matrix\n",
    "    train_labels = numpy.concatenate((numpy.full(len(featuresTrain[0]), 1, dtype=int), numpy.full(len(featuresTrain[1]), -1, dtype=int)))  # Training labels\n",
    "    test_fm = numpy.concatenate((featuresTest[1], featuresTest[3]))  # Testing feature matrix\n",
    "    test_labels = numpy.concatenate((numpy.full(len(subsets[1]), 1, dtype=int), numpy.full(len(subsets[3]), -1, dtype=int)))  # Testing labels\n",
    "\n",
    "    #classifier = naive_bayes.GaussianNB()  # Creating model object\n",
    "    classifier = tree.DecisionTreeClassifier(random_state=1) # Creating model object\n",
    "    classifier.fit(train_fm, train_labels)  # Training model\n",
    "\n",
    "\n",
    "\n",
    "    predicted = classifier.predict(test_fm)  # Using model to predict labels of testing data\n",
    "\n",
    "    accuracy = metrics.accuracy_score(test_labels, predicted)  # Computing accuracy:\n",
    "\n",
    "    counter = 0\n",
    "    bound = 1.01\n",
    "    boundFreq = len(task.patterns) +1\n",
    "    while(counter < k ) :\n",
    "        pat = maxconf(task,bound)\n",
    "        patfreq = maxFreq(pat,boundFreq)\n",
    "        while(len(patfreq)> 0 and counter < k):\n",
    "            counter = counter + 1\n",
    "            pos = patfreq[0][1]\n",
    "            neg = patfreq[0][2]\n",
    "            bound = pos / (pos+neg)\n",
    "            bound = bound - 10e-18\n",
    "            for elem in patfreq:\n",
    "                print('{} {} {}'.format(elem[0],(elem[1] / (elem[1]+elem[2])),(elem[1]+elem[2])))\n",
    "            freq = pos + neg\n",
    "            patfreq = maxFreq(pat,freq)\n",
    "\n",
    "    # printing classification results:\n",
    "    print(predicted.tolist())\n",
    "    print('accuracy: {}'.format(accuracy))\n",
    "    print()  # Blank line to indicate end of fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [a,b,d,r]\n",
    "list2 = [a,b,x,r]\n",
    "\n",
    "def remove(list1, list2):\n",
    "    for elem in range(len(list1)):\n",
    "        list_1, list_2 = list1[elem], list2[elem]\n",
    "        for item in list_1:\n",
    "            list_2.remove(item)\n",
    "    return list2\n",
    "new= remove(list1, list2)\n",
    "new"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python38364bit7f1e8114d39d4554aac99694211e46b7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
